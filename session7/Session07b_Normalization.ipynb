{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "Normalization can happen in various places in the Gothenburg model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../ancillary/gotModel.png\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization 1. Near match in the alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Near matching is another term for fuzzy matching, based on the idea that two items (such as two word tokens in a collation) should sometimes be considered matching even when they are not string equal (that is, not identical in every character). More precisely, near matching is a strategy for finding the closest match in situations where they not be an exact match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collatex import *\n",
    "collation = Collation()\n",
    "collation.add_plain_witness('Witness 1', \"N'oublie pas cette importante nouvelle\")\n",
    "collation.add_plain_witness('Witness 2', \"N'oublie pas cette poubelle\")\n",
    "result = collate(collation, segmentation=False, near_match=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When CollateX cannot find an exact match and there is more than one possible alignment for a token, it defaults to placing the token to the left. With near matching, though, CollateX can recognize that, in the example above, *poubelle* is closer to *nouvelle* and thus place it in the right column.\n",
    "\n",
    "After the alignment, if \n",
    "1. there is no exact match and \n",
    "2. the number of tokens of the witnesses is not the same\n",
    "CollateX will compare all possible matches and calculates the nearest match using a measure called *edit distance* or *Levenshtein distance*.\n",
    "\n",
    "Performing computationally inexpensive exact string matching first (in the alignment stage) and then calculating the more expensive edit distance (in the analysis stage) only where alignment has failed to give a satisfactory answer reduces the amount of computation required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Your turn!*\n",
    "\n",
    "Insert a new code cell here below (using the button +) and collate the sentences \"The big gray koala\" and \"The grey koala\", with the near_match parameter set to True or to False. Observe the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization 2. Edit the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will use a sonnet about writing a sonnet by Lope de Vega, in a French translation.\n",
    "\n",
    "The output of the collation mostly contains differences of punctuation and capitalization, as we can see in the results here below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collatex import *\n",
    "collation = Collation()\n",
    "witness_1707 = open( \"../data/Lope/Lope_soneto_FR_1707.txt\", encoding='utf-8' ).read()\n",
    "witness_1822 = open( \"../data/Lope/Lope_soneto_FR_1822.txt\", encoding='utf-8' ).read()\n",
    "collation.add_plain_witness( \"wit 1707\", witness_1707 )\n",
    "collation.add_plain_witness( \"wit 1822\", witness_1822 )\n",
    "alignment_table = collate(collation, output='html2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we are not interested in punctuation and capitalization: we only want what might be called 'substantive variants'.\n",
    "\n",
    "The \"hard way\" of obtaining the expected result is to **remove punctuation and lower-case in all the texts**. The code below will do just that: it will\n",
    "- create a new directory, inside the `data/Lope` dir, called `norm`\n",
    "- make a normalized copy (without punctuation and all lower-case) of each file inside the new `norm` dir\n",
    "\n",
    "The creation of a normalized copy is safer than just normalizing the original transcriptions. If you keep the originals, you can always come back to them and perform other kinds of normalization if needed.\n",
    "\n",
    "**Note**: the code below contains lots of comments, that is string that will not be executed but can be used for documentation. As we've seen yesterday, in Python comments are marked with the sign #."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages to work with files and for regular expressions\n",
    "import glob, re, os\n",
    "\n",
    "path = '../data/Lope/'  # put the path into a variable \n",
    "\n",
    "os.makedirs(path + 'norm', exist_ok=True)  # create a new folder, if does not exist\n",
    "\n",
    "files = [os.path.basename(x) for x in glob.glob(path+'*.txt')]  # take all txt files in the directory\n",
    "\n",
    "for file in files:  # for each file in the directory\n",
    "    \n",
    "    ### READ THE FILE CONTENT\n",
    "    file_opened = open(path+file, 'r', encoding='utf-8') # open the file in mode 'r' (read)\n",
    "    content = file_opened.read()  # read the file content\n",
    "    \n",
    "    ### ALL TO LOWER CASE\n",
    "    lowerContent = content.lower() \n",
    "    \n",
    "    ### REMOVE PUNCTUATION \n",
    "    # replace everything that is not alphanumeric character (\\w) or space (\\s) with a whitespace\n",
    "    noPunct_lowerContent = re.sub(r'[^\\w\\s]',' ',lowerContent) \n",
    "    \n",
    "    ### REMOVE MULTIPLE WHITESPACES\n",
    "    regularSpaces_noPunct_lowerContent = \" \".join(noPunct_lowerContent.split())\n",
    "    \n",
    "    ### CREATE A NEW FILE\n",
    "    filename = file.split('.')[0]\n",
    "    new_file = open(path+'norm/' + filename + '_norm.txt', 'w', encoding='utf-8') # open the new file in mode 'w' (write)\n",
    "    \n",
    "    ### WRITE THE NEW CONTENT INTO THE NEW FILE\n",
    "    new_file.write(regularSpaces_noPunct_lowerContent) \n",
    "    \n",
    "    ### CLOSE THE FILE\n",
    "    new_file.close()\n",
    "    \n",
    "print('Finished! All normalized!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's **collate the normalized copies**.\n",
    "\n",
    "Attention to the new path. The output should be different from the one above!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collatex import *\n",
    "collation = Collation()\n",
    "witness_1707 = open( \"../data/Lope/norm/Lope_soneto_FR_1707_norm.txt\", encoding='utf-8' ).read()\n",
    "witness_1822 = open( \"../data/Lope/norm/Lope_soneto_FR_1822_norm.txt\", encoding='utf-8' ).read()\n",
    "collation.add_plain_witness( \"wit 1707\", witness_1707 )\n",
    "collation.add_plain_witness( \"wit 1822\", witness_1822 )\n",
    "alignment_table = collate(collation, output='html2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization 3. Annotate the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to prepare the texts for collation is enriching them with annotations: the annotation for each token will be regarded as the normalized form by the software and be used for collation. \n",
    "\n",
    "This approach is only possible because CollateX has a fundamental characteristic. CollateX stores by default two information for each token, called 't' and 'n'. They have been created precisely to handle normalization issues. The 't' property stores the original token, while the 'n' property can store a normalized form of 't'. CollateX used the string recorded into the 'n' property to collate, always. When a 'n' property is not explicitey defined, the value of the 't' property is copied into 'n'.\n",
    "\n",
    "We can put whatever we want into 'n'. As in the example above, we can record there a copy of each token, but lower-case and without punctuation. We can also do more sophisticated processing and use 'n' to store linguistic information for each token, to be reused during the alignment phase or for analysis.\n",
    "\n",
    "Let's go back to the previous example and only consider the first tercet. We'll see that the only differences are punctuation signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first tercet only\n",
    "\n",
    "from collatex import *\n",
    "collation = Collation()\n",
    "collation.add_plain_witness( \"wit 1707\", \"Je commence au hasard; et si je ne m'abuse,\")\n",
    "collation.add_plain_witness( \"wit 1822\", \"Je commence au hasard, et, si je ne m'abuse,\")\n",
    "collate(collation, output='html2', segmentation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to arrive at the same results that we reached in Normalization 1, but using the 't' and 'n' properties. They become visible if we input the data for collation as JSON.\n",
    "\n",
    "In the example below, you can see that there are two witnesses, each having an \"id\" and some \"tokens\". For each token, the 't' and 'n' properties are recorded.\n",
    "\n",
    "In this case, we manually store the value we prefer into the 'n' values. Remember, it is not important what exactly it is, but that it indicates what we want.\n",
    "\n",
    "Try to run the cell below!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Your turn!*\n",
    "\n",
    "Change the value of the 'n' property for some tokens and re-run the cell to see what happens. For example, the fourth token is \"hasard;\" in the first witness and \"hasard,\" in the second witness. Replace both their \"n\" values with \"random\" (the English translation of *au hasard*) and re-run the cell. The result should not change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first tercet only\n",
    "# normalize with nothing will give errors in the svg output\n",
    "\n",
    "from collatex import *\n",
    "import json\n",
    "collation = Collation()\n",
    "json_input = \"\"\"{\n",
    "    \"witnesses\": [\n",
    "        {\n",
    "            \"id\": \"wit1707\",\n",
    "            \"tokens\": [\n",
    "                {\n",
    "                    \"t\": \"Je\",\n",
    "                    \"n\": \"je\" \n",
    "                },\n",
    "                {\n",
    "                    \"t\": \"commence\",\n",
    "                    \"n\": \"commence\"\n",
    "                },\n",
    "                {\n",
    "                    \"t\": \"au\",\n",
    "                    \"n\": \"au\"\n",
    "                },\n",
    "                {\n",
    "                    \"t\": \"hasard;\",\n",
    "                    \"n\": \"hasard\"\n",
    "                },\n",
    "                {\n",
    "                    \"t\": \"et\",\n",
    "                    \"n\": \"et\"\n",
    "                },\n",
    "                {\n",
    "                    \"t\": \"si\",\n",
    "                    \"n\": \"si\"\n",
    "                },\n",
    "                {\n",
    "                    \"t\": \"je\",\n",
    "                    \"n\": \"je\"\n",
    "                },\n",
    "                {\n",
    "                    \"t\": \"ne\",\n",
    "                    \"n\": \"ne\"\n",
    "                },\n",
    "                {\n",
    "                    \"t\": \"m'abuse,\",\n",
    "                    \"n\": \"m'abuse\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"wit1822\",\n",
    "            \"tokens\": [\n",
    "                {\n",
    "                    \"t\": \"Je\",\n",
    "                    \"n\": \"je\"\n",
    "                },\n",
    "                {\n",
    "                    \"t\": \"commence\",\n",
    "                    \"n\": \"commence\"\n",
    "                },\n",
    "                {\n",
    "                    \"t\": \"au\",\n",
    "                    \"n\": \"au\"\n",
    "                },\n",
    "                {\n",
    "                    \"t\": \"hasard,\",\n",
    "                    \"n\": \"hasard\"\n",
    "                },\n",
    "                {\n",
    "                    \"t\": \"et,\",\n",
    "                    \"n\": \"et\"\n",
    "                },\n",
    "                {\n",
    "                    \"t\": \"si\",\n",
    "                    \"n\": \"si\"\n",
    "                },\n",
    "                {\n",
    "                    \"t\": \"je\",\n",
    "                    \"n\": \"je\"\n",
    "                },\n",
    "                {\n",
    "                    \"t\": \"ne\",\n",
    "                    \"n\": \"ne\"\n",
    "                },\n",
    "                {\n",
    "                    \"t\": \"m'abuse,\",\n",
    "                    \"n\": \"m'abuse\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\"\"\"\n",
    "collate(json.loads(json_input), segmentation=False, output=\"svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is wonderful. But very time consuming!\n",
    "\n",
    "We can automatically assign a value to the \"n\" property, processing in some way the \"t\" value. In the example below, the \"n\" value is a copy of the \"t\" value where all letters are lower-case and the punctuation is removed.\n",
    "\n",
    "In the results, you will see the json input, followed by CollateX output. If you only want the output, in the code below comment out\n",
    "    \n",
    "    print(json_input)\n",
    "\n",
    "by adding # at the beginning of the line, as in\n",
    "\n",
    "    # print(json_input)\n",
    "\n",
    "And re-run the cell!\n",
    "\n",
    "**Note**: when the input is given in json, if *segmentation* is set to *True* there are no whitespaces between words, because the input are single tokens without whitespaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collatex import *\n",
    "import json\n",
    "\n",
    "witness_1707 = open( \"../data/Lope/Lope_soneto_FR_1707.txt\", encoding='utf-8' ).read()\n",
    "witness_1822 = open( \"../data/Lope/Lope_soneto_FR_1822.txt\", encoding='utf-8' ).read()\n",
    "\n",
    "A = [\"wit 1707\", witness_1707]\n",
    "B = [\"wit 1822\", witness_1822]\n",
    "\n",
    "listWitnesses = [A,B]   # create a list of witnesses\n",
    "\n",
    "data = {}\n",
    "data[\"witnesses\"] = []\n",
    "for witness in listWitnesses: # for each witness in the list\n",
    "    tokens = []   # create empty list for tokens\n",
    "    data[\"witnesses\"].append({\n",
    "        \"id\": witness[0],  # give as id the first item in A or B\n",
    "        \"tokens\" : tokens  # and as tokens the empty list\n",
    "    })\n",
    "    for w in witness[1].split():  # for each word in witness (second item in A or B)\n",
    "        t = w   # t is the original word\n",
    "        # We want n to be the original word (w) in lower-case and without punctuation.\n",
    "        # How do we achieve this? We replace everything that is not alphanumeric character (\\w) or space (\\s) with nothing. \n",
    "        # Attention: if replaced with whitespace, it will create differences --> avoid.\n",
    "        # This does not happen in the previous method (Normalization 1), \n",
    "        # because the tokenization happens afterwards and strip whitespaces.\n",
    "        n = re.sub(r'[^\\w\\s]','',w.lower()) \n",
    "        tokens.append({    # populate the empty token list with values for t and n\n",
    "            \"t\" : t,\n",
    "            \"n\" : n\n",
    "        })\n",
    "        \n",
    "json_input = json.dumps(data)  # data created turned into json string with double quotes\n",
    "# print(json_input)\n",
    "\n",
    "collation = Collation()\n",
    "# if segmentation=True there are no whitespaces between words, because input is given with single tokens without whitespaces\n",
    "collate(json.loads(json_input), segmentation=False, output=\"html2\")\n",
    "collate(json.loads(json_input), segmentation=False, output=\"svg\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
